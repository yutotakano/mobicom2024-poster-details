---
layout: ../layouts/Layout.astro
title: On The Public Cloud Deployment of Cloud-Native Mobile Core Systems
description: Leveraging the public cloud for the Core Network (CN) remains uncommon amongst mobile network operators, despite the economical and operational benefits. We present a holistic analysis of the public cloud in comparison to on-site mobile core deployments, showing the technical and economical feasibility, and laying the ground for further adoption of the public cloud.
favicon: favicon.svg
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import LaTeX from "../components/LaTeX.astro";

import challenges from "../assets/challenges.svg?raw";
import latencies from "../assets/latencies.svg?raw";
import vcpus from "../assets/vcpus.svg?raw";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Yuto Takano",
      url: "\\nL^),k3S5id}HswFav^",
      notes: ["†"],
    },
    {
      name: "Andrew E. Ferguson",
      notes: ["†"],
    },
    {
      name: "Mahesh K. Marina",
      notes: ["†"],
    },
  ]}
  conference="ACM MobiCom 2024 (Nov 18—22th, 2024)"
  notes={[
    {
      symbol: "†",
      text: "The University of Edinburgh",
    },
  ]}
  links={[
    {
      name: "Poster PDF",
      url: "MobiCom_2024_Public_Cloud_Deployment_Poster.pdf",
      icon: "fa-solid:file-pdf",
    },
  ]}
  />

<HighlightedSection>

## Overview

Leveraging the public cloud for the Core Network (CN) remains uncommon amongst mobile network operators, despite the economical and operational benefits. The drive to "cloud-native", through advancements in mobile core architecture research and standard evolution have made the case stronger.

We believe that there are four types of challenges inhibiting this adoption: ❶ Jurisdictional regulations, ❷ Cost Transparency, ❸ Safety and Control, and ❹ 3GPP & SLA compliance. In addition, all of the aforementioned challenges apart from the first require in-depth observability, which would be hard to achieve without direct control of the hardware.

While some past works have analyzed limited aspects of these challenges [[1]], our findings are the first to provide a holistic analysis of the public cloud in comparison to on-site mobile core deployments. Our findings show the technical and economical feasibility, and lay the ground for further adoption of the public cloud.

</HighlightedSection>

## Challenges

The following is a diagram illustrating the challenges inhibiting the adoption of the public cloud for 5G CNs. This matches industry survey results [[2]].

<div style="width: 100%;margin: 1rem 0 3rem 0" set:html={challenges} />

## Previous Work

Previous works have examined the reliability [[1]] and data privacy of the public cloud [[2]]. Our's is the first to provide a holistic analysis including technical measurements and relative cost estimations.

In industry, Mobi in 2022 put some of their spectrum access system of their 5G mobile core to AWS [[8]]. In 2024, O2 Telefónica announced their move to AWS for their core as well [[9]]. These announcements exemplify the industry's attention towards moving the core to the public cloud to achieve cost savings. The O2 Telefónica case is particularly informative, as the press release explains the specific technologies used: AWS Elastic Kubernetes Service for orchestration, AWS Elastic Compute Cloud VMs for virtualization, AWS Direct Connect for connecting to on-premise software, and AWS Nitro System for security.
Other major network operators such as Three have migrated their non-critical IT workloads to the public cloud [[10]], building knowledge internally to prepare for full migration.

The appeals of the public cloud are clear from these cases. Our work is the first of its kind to extensively analyze this deployment strategy, with an aim to uncover the benefits, operational challenges, and improvements that need addressing for further adoption.


## Setup

We run our evaluation and experiments on the following setup:

- Public Cloud: AWS. AWS' plans are representative of all public hypercloud offers, with options for managed Kubernetes control-planes, managed VMs, serverless functionality, managed databases, etc.
- CN software: CoreKube [[3]], which is a cloud-native mobile core system. It was chosen for being representative of the direction CN software are heading. Other similar works include ML-SLD [[4]], Proc5GC [[5]], and PP5GS [[6]].
- RAN traffic load: Nervion RAN emulator [[7]].

## Deployment Methods

<div class="flex flex-wrap gap-4 w-full">
  <div class="flex-1 min-w-[16rem]">
    <h3 class="mb-3">1. Elastic Kubernetes Service (EKS) with Fargate instances</h3>
    <p class="text-base">EKS is AWS's managed Kubernetes control-plane. Fargate is AWS's compute engine technology for cloud-native software, which runs each container in its own ephemeral micro VM. The equivalent on GCP and Azure are "Cloud Run" and "Container Apps". EKS costs $0.1/hour, and Fargate costs $0.0133/vCPU/hour for compute and $0.00147/GB/hour for memory respectively.</p>
  </div>
  <div class="flex-1 min-w-[16rem]">
    <h3 class="mb-3">2. EKS with Elastic Compute Cloud (EC2) instances</h3>
    <p class="text-base">EKS can be configured to use standard EC2 VMs for container scheduling too. EC2 costs vary largely, but for CPU-bound software like the CN, machines like `t3.medium` at $0.0418 for 2 vCPU and 4 GB memory is enough.</p>
  </div>
  <div class="flex-1 min-w-[16rem]">
    <h3 class="mb-3">3. Self-managed Kubernetes on EC2 instances</h3>
    <p class="text-base">One can also host their own Kubernetes control-plane within a EC2 VM too, and manage its configuration entirely themselves.</p>
  </div>
</div>


## Qualitative Analysis

<div class="w-full overflow-x-scroll">
  <table class="w-full border-collapse">
    <thead>
      <tr>
        <th></th>
        <th class="bg-red-800 text-white p-2">
          <span style={{ fontSize: "larger" }}>EKS with Fargate</span> <br /> <span>(unsuited)</span>
        </th>
        <th class="bg-green-800 text-white p-2">
          <span style={{ fontSize: "larger" }}>EKS with EC2</span> <br /> <span>(feasible)</span>
        </th>
        <th class="bg-gray-400 text-white p-2">
          <span style={{ fontSize: "larger" }}>Self-hosted K8S on EC2</span> <br /> <span>(suboptimal)</span>
        </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="text-lg font-bold">
          Flexibility in Configuration
        </td>
        <td class="align-top">
          <span class="bg-red-800 text-white font-bold px-1">Worst:</span> Fargate's minimum VM size is 0.25 vCPU 0.5 GB. This leads to underutilisation for cloud-native CNs like CoreKube, which handles larger demands by replicating many lightweight containers.
        </td>
        <td class="align-top">
          <span class="bg-green-800 text-white font-bold px-1">Best:</span> A standard VM such as <code>t3.medium</code> can fit six to seven Fargate-size pods. With CoreKube-like architectures, one can run even more pods per instance for higher performance at the same price.
        </td>
        <td class="align-top">
          <span class="text-green-800 font-bold">Good:</span> The full range of EC2 VMs is available to choose from, and you can pack pods for cost efficiency. However, this setup requires extra fault-tolerant VMs for the k8s control plane to achieve HA.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#fce7cb" }}>
        <td class="text-lg font-bold">
          Migration Cost
        </td>
        <td class="align-top">
          <span class="bg-red-800 font-bold text-white px-1">Worst:</span> Software required to adapt to managed k8s control plane and VM abstraction.
        </td>
        <td class="align-top">
          <span class="text-red-800 font-bold">Good:</span> Requires software changes but only to adapt to the managed k8s control plane.
        </td>
        <td class="align-top">
          <span class="bg-green-800 font-bold text-white px-1">Best:</span> No software changes required compared to on-prem.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#fce7cb" }}>
        <td class="text-lg font-bold">
          Service Cost
        </td>
        <td class="align-top">
          <span class="text-green-800 font-bold">Good:</span> EKS cost, plus pricing for Fargate compute and memory based pricisely on usage.
        </td>
        <td class="align-top">
          <span class="bg-green-800 font-bold text-white px-1">Best:</span> EKS cost, plus EC2 VM cost. EC2 Spots achieve better price-per-pod than Fargate.
        </td>
        <td class="align-top">
          <span class="text-green-800 font-bold">Good:</span> Only EC2 VM cost. But high availability control-plane puts costs about equal as EKS.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#fce7cb" }}>
        <td class="text-lg font-bold">
          Personnel Cost
        </td>
        <td class="align-top">
          <span class="bg-green-800 font-bold text-white px-1">Best:</span> Everything is managed by AWS, little maintenance manpower required.
        </td>
        <td class="align-top">
          <span class="text-green-800 font-bold">Good:</span> Operators only need to manage workload nodes.
        </td>
        <td class="align-top">
          <span class="bg-red-800 text-white font-bold px-1">Worst:</span> Operators need knowledge and skills to manage control plane and workload nodes.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#dbcee6" }}>
        <td class="text-lg font-bold">
          Debug Privileges
        </td>
        <td class="align-top">
          <span class="bg-red-800 font-bold text-white px-1">Worst:</span> Without root access on the micro VMs hosting Fargate pods, it is very difficult to trace syscalls or debug issues.
        </td>
        <td class="align-top">
          <span class="bg-green-800 font-bold text-white px-1">Best:</span> Privileged access on VMs for control, while removing the noise that is the k8s control plane.
        </td>
        <td class="align-top">
          <span class="text-green-800 font-bold">Good:</span> Control over everything, which can lead to more unnecessary sources of error.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#dbcee6" }}>
        <td class="text-lg font-bold">
          Software Isolation
        </td>
        <td class="align-top">
          <span class="bg-green-800 text-white font-bold px-1">Best:</span> Micro-VMs isolate each container.
        </td>
        <td class="align-top">
          <span class="text-red-800 font-bold">Bad:</span> Isolation is entirely up to the operator.
        </td>
        <td class="align-top">
          <span class="text-red-800 font-bold">Bad:</span> Isolation is entirely up to the operator.
        </td>
      </tr>
      <tr style={{ backgroundColor: "#d6e8fd" }}>
        <td class="text-lg font-bold">
          Protocol Compliance
        </td>
        <td class="align-top">
          <span class="bg-red-800 text-white font-bold px-1">Worst:</span> Requires a proxy since Fargate does not support SCTP port 38412 (NGAP).
        </td>
        <td class="align-top">
          <span class="bg-green-800 text-white font-bold px-1">Best:</span> Ports work as-is as long as you allow it in the firewall.
        </td>
        <td class="align-top">
          <span class="bg-green-800 text-white font-bold px-1">Best:</span> Ports work as-is as long as you allow it in the firewall.
        </td>
      </tr>
    </tbody>
  </table>
</div>


## Cost Efficiency Validation

<div class="w-full" set:html={vcpus} />

The compute (vCPU) usage per worker pod under a load of 100 connected UEs shows that Fargate instances are not efficient in their resource use.

※ Note that the EC2 results apply for both EKS and Self-Managed control plane deployments.

## SLA Conformance Validation

<div class="w-full" set:html={latencies} />

The control-plane message processing latency shows Fargate as being unacceptably slow and inconsistent w.r.t the allowed end-to-end latency. This 'allowed latency' value is not a precise requirement from 3GPP, but a recommended value [[11]].

※ Note that the EC2 results apply for both EKS and Self-Managed control plane deployments.

## Future Work

- **Observability Solution.** Underpinning all the challenges featured in our work is `Observability', the ability to monitor the state of each and automatically or manually act. Our future work is to propose a software solution to improve observability in cloud-native CN software specifically for public cloud use cases.
- **Dataplane Feasibility.** The dataplane is subject to stricter latency and reliability requirements than the control plane. Further work is required to determine the feasibility of the public cloud for the dataplane, and otherwise design architectures to overcome any challenges.

## Conclusions

Our poster explored and demonstrated the feasibility of deploying a cloud-native mobile core network to public cloud infrastructure, from a holistic perspective encompassing ease of manageability, cost, and performance. We performed detailed qualitative analysis of estimated cost, technical challenges, manageability and observability of each deployment option. We then performed experimental validation of message latency and cost efficiency, and concluded that a public cloud deployment is feasible from a cost and performance perspective. Our recommend deployment method is to use a managed K8S control plane like EKS, together with VMs like EC2. 

We further noticed the lack of good observability into the challenges and criteria highlighted in this poster, which could hinder adoption of both 5G and public cloud deployment in mobile network operators. A solution to this will be presented in our next work.

[1]: https://dl.acm.org/doi/abs/10.1145/3241539.3241564
[2]: https://www.analysysmason.com/contentassets/159c180ce38c4bc6bd198949c9af9b42/analysys_mason_sovereign_cloud_5g_perspective_feb2023_rma16.pdf
[3]: https://dl.acm.org/doi/10.1145/3570361.3592522
[4]: https://doi.org/10.1016/j.dcan.2022.04.026
[5]: https://doi.org/10.1109/BlackSeaCom58138.2023.10299772
[6]: https://doi.org/10.1109/TNSM.2022.3230206
[7]: https://dl.acm.org/doi/10.1145/3447993.3483248
[8]: https://www.federatedwireless.com/news/federated-wireless-mobi-cbrs-deployment
[9]: https://aws.amazon.com/blogs/industries/o2-telefonica-moves-its-5g-core-network-to-the-cloud-with-aws-and-nokia/
[10]: https://www.lightreading.com/cloud/the-public-cloud-has-failed-to-crack-telecom
[11]: https://opennetworking.org/wp-content/uploads/2022/05/Jingqi-Huang-and-Jiayi-Meng-Final-Slide-Deck.pdf
